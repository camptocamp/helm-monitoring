---
# Source: monitoring/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-monitoring-env
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: main
data:
  CHART_NAME: monitoring
  RELEASE_NAME: test
  RELEASE_NAMESPACE: default
  DBSTATS_CLIENT_RELEASE: "latest"
---
# Source: monitoring/templates/info-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-monitoring-capabilities-helm
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: main
data:
  git_commit: 7ceeda6c585217a19a1131663d8cd1f7d641b2a7
  git_tree_state: clean
  go_version: go1.17.5
  version: v3.9.0
---
# Source: monitoring/templates/info-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-monitoring-capabilities-kube
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: main
data:
  Major: "1"
  Minor: "24"
  Version: v1.24.0
---
# Source: monitoring/templates/dbstats-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: test-monitoring-dbstats
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: dbstats
spec:
  schedule: "32 * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            app.kubernetes.io/name: monitoring
            app.kubernetes.io/instance: test
            app.kubernetes.io/component: dbstats
        spec:
          serviceAccountName: full
          securityContext:
            null
          affinity:
            {}
          restartPolicy: Never
          containers:
            - name: dbstats
              securityContext:
                runAsNonRoot: true
                runAsUser: 33
              image: "camptocamp/c2cwsgiutils:latest"
              imagePullPolicy: IfNotPresent
              env:
                - name: "C2C_PROMETHEUS_PORT"
                  value: "9090"
                - name: "LOG_LEVEL"
                  value: INFO
                - name: "PGDATABASE"
                  valueFrom:
                    secretKeyRef:
                      name: "database"
                      key: "database"
                - name: "PGHOST"
                  valueFrom:
                    secretKeyRef:
                      name: "database"
                      key: "hostnameSlave"
                - name: "PGPASSWORD"
                  valueFrom:
                    secretKeyRef:
                      name: "database"
                      key: "password"
                - name: "PGPORT"
                  valueFrom:
                    secretKeyRef:
                      name: "database"
                      key: "portSlave"
                - name: "PGUSER"
                  valueFrom:
                    secretKeyRef:
                      name: "database"
                      key: "username"
              terminationMessagePolicy: FallbackToLogsOnError
              resources:
                limits:
                  memory: 2.5Mi
                requests:
                  cpu: 1m
                  memory: 1.5Mi
              args:
                - bash
                - -cxe
                - |
                  c2cwsgiutils-stats-db \
                    --db=postgresql://$(PGUSER):$(PGPASSWORD)@$(PGHOST):$(PGPORT)/test1 \
                  c2cwsgiutils-stats-db \
                    --db=postgresql://test2:test2@test2:6789/test2 \
              ports:
                - name: prometheus
                  containerPort: 9090
                  protocol: TCP
---
# Source: monitoring/templates/dbstats-podmonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: test-monitoring-dbstats
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: dbstats
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: monitoring
      app.kubernetes.io/instance: test
      app.kubernetes.io/component: dbstats
  podMetricsEndpoints:
    - honorLabels: true
      interval: 10s
      port: prometheus
---
# Source: monitoring/templates/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: test-monitoring
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: main
    prometheus: example
    role: alert-rules
spec:
  groups:
    - name: geo_status_c2cwsgiutils_default
      rules:
        - record: geo_federation:status_k8s
          labels:
            kind: health_check_fail
          expr: |
            max by (namespace, chart, release) (
              (
                increase(
                  statsd_route_seconds_count{namespace="default", route="c2c_health_check", group="5"}[5m]
                )
              ) > bool 0 * 10
              or
              # just to have the entry when there is no error
              statsd_route_seconds_count{namespace="default", route="c2c_health_check"} * 0
            )

        - record: geo_federation:status_k8s
          labels:
            kind: slow_sql
          expr: |
            max by (namespace, chart, release) (
              max_over_time(
                statsd_sql_seconds{namespace="default", quantile="0.99"}[5m]
              )
            ) > bool 10

        - record: geo_federation:const_k8s
          labels:
            kind: slow_sql
          expr: |
            max by (namespace) (
              statsd_route_seconds_count{namespace="default"} > bool 0
            ) * 10

        - record: geo_federation:status_k8s
          labels:
            kind: versions
          expr: |
            max by (namespace, chart, release) (
              (
                count by (namespace, chart, release, service) (
                  increase(statsd_version{namespace="default"}[5m]) > 0
                ) > bool 1
              ) * 1
            )

        - record: geo_federation:status_k8s
          labels:
            kind: slow_fetches
          expr: |
            max by (namespace, chart, release) (
              (
                max_over_time(
                  statsd_requests_seconds{quantile="0.99", namespace="default"}[5m]
                ) > bool 20
              ) * 1
            )

        - record: geo_federation:const_k8s
          labels:
            kind: slow_fetch
          expr: |
            max by (namespace) (
              statsd_route_seconds_count{namespace="default"} > bool 0
            ) * 20

        - record: geo_federation:status_k8s
          labels:
            kind: slow_routes
          expr: |
            max by (namespace, chart, release) (
              (
                max_over_time(
                  statsd_route_seconds{quantile="0.99", namespace="default"}[5m]
                ) > bool 90
              ) * 1
            )

        - record: geo_federation:const_k8s
          labels:
            kind: slow_routes
          expr: |
            max by (namespace) (
              statsd_route_seconds_count{namespace="default"} > bool 0
            ) * 90

        - record: geo_federation:status_k8s
          labels:
            kind: tilecloud_errors
          expr: |
            max by (namespace, chart, release) (
              (
                increase(
                  statsd_counter{namespace="default", service="tilecloudchain", statsd=~"error\\..*"}[5m]
                ) > bool 0
              )
              or
              # just to have the entry when there is no error
              statsd_timer_seconds_count{namespace="default", service="tilecloudchain"} * 0
            )

        - record: geo_federation:status_k8s
          labels:
            kind: scm_errors
          expr: |
            max by (namespace, chart, release) (
              (
                increase(
                  statsd_counter{namespace="default", service=~".*_config|config_api", statsd=~".*error"}[5m]
                ) > bool 0
              )
              or
              # just to have the entry when there is no error
              statsd_timer_seconds_count{namespace="default", service=~".*_config|config_api"} * 0
            )
---
# Source: monitoring/templates/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: test-monitoring
  labels:
    helm.sh/chart: monitoring
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/instance: test
    app.kubernetes.io/component: main
    prometheus: example
    role: alert-rules
spec:
  groups:
    - name: geo_status_print_default
      rules:
        - record: geo_federation:status_k8s
          labels:
            kind: print_gc
          expr: |
            (
              max by (namespace, chart, release) (
                # if we take more than 50% of the time doing GC
                sum by (namespace, chart, release, pod_name) (
                  rate(
                    statsd_print{namespace="default", metric=~"print\\.jvm-gc.*\\.time"}[5m]
                  )
                ) > bool 500 * 1
                +
                # if we take more than 95% of the time doing GC
                sum by (namespace, chart, release, pod_name) (
                  rate(
                    statsd_print{namespace="default", metric=~"print\\.jvm-gc.*\\.time"}[5m]
                  )
                ) > bool 950 * 9
              )
            ) * 1

        - record: geo_federation:status_k8s
          labels:
            kind: print_errors
          expr: |
            sum by (namespace, chart, release) (
              increase(
                statsd_print{namespace="default", metric=~"print\\.org\\.mapfish\\.print\\..*\\.error"}[5m]
              )
            ) > bool 0 * 1
            or
            # just to have the entry when there is no error
            sum by (namespace, chart, release) (
              statsd_print{namespace="default"}
            ) * 0

        - record: geo_federation:status_k8s
          labels:
            kind: print_overflows
          expr: |
            sum by (namespace, chart, release) (
              increase(
                statsd_print{namespace="default", metric=~"print\\.org\\.mapfish\\.print\\..*\\.queue_overflow"}[5m]
              )
            ) > bool 0 * 1
            or
            # just to have the entry when there is no error
            sum by (namespace, chart, release) (
              statsd_print{namespace="default"}
            ) * 0

        - record: geo_federation:status_k8s
          labels:
            kind: print_queue_times
          expr: |
            avg by (namespace, chart, release) (
              statsd_print{namespace="default", metric=~"print\\.org\\.mapfish\\.print\\..*\\.wait\\.mean"}
            ) > bool 10*1000 * 1
            or
            # just to have the entry when there is no error
            sum by (namespace, chart, release) (
              statsd_print{namespace="default"}
            ) * 0
